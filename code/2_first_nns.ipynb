{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks on bacteria dataset\n",
    "Notebook with first NN (LLMs helped) runs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ramanspy as rp\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapt GPT base nn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 1.1572, Val Loss: 3.7897, Val Acc: 0.3183\n",
      "Epoch [2/20], Train Loss: 0.5547, Val Loss: 5.1153, Val Acc: 0.2957\n",
      "Epoch [3/20], Train Loss: 0.4326, Val Loss: 4.0506, Val Acc: 0.4427\n",
      "Epoch [4/20], Train Loss: 0.3673, Val Loss: 5.3674, Val Acc: 0.3720\n",
      "Epoch [5/20], Train Loss: 0.3315, Val Loss: 5.9996, Val Acc: 0.3570\n",
      "Epoch [6/20], Train Loss: 0.3023, Val Loss: 5.7974, Val Acc: 0.4103\n",
      "Epoch [7/20], Train Loss: 0.2793, Val Loss: 6.4810, Val Acc: 0.3973\n",
      "Epoch [8/20], Train Loss: 0.2652, Val Loss: 6.3177, Val Acc: 0.3803\n",
      "Epoch [9/20], Train Loss: 0.2486, Val Loss: 6.5065, Val Acc: 0.3697\n",
      "Epoch [10/20], Train Loss: 0.2368, Val Loss: 8.3532, Val Acc: 0.3387\n",
      "Epoch [11/20], Train Loss: 0.2317, Val Loss: 8.2538, Val Acc: 0.3747\n",
      "Epoch [12/20], Train Loss: 0.2150, Val Loss: 7.6237, Val Acc: 0.3737\n",
      "Epoch [13/20], Train Loss: 0.2132, Val Loss: 7.1958, Val Acc: 0.4043\n",
      "Epoch [14/20], Train Loss: 0.1993, Val Loss: 8.2469, Val Acc: 0.3683\n",
      "Epoch [15/20], Train Loss: 0.1969, Val Loss: 10.1581, Val Acc: 0.3277\n",
      "Epoch [16/20], Train Loss: 0.1883, Val Loss: 9.8009, Val Acc: 0.3460\n",
      "Epoch [17/20], Train Loss: 0.1828, Val Loss: 9.3597, Val Acc: 0.3590\n",
      "Epoch [18/20], Train Loss: 0.1783, Val Loss: 11.6863, Val Acc: 0.3240\n",
      "Epoch [19/20], Train Loss: 0.1785, Val Loss: 10.7381, Val Acc: 0.3413\n",
      "Epoch [20/20], Train Loss: 0.1712, Val Loss: 10.4126, Val Acc: 0.3553\n",
      "Test Accuracy: 0.3377\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing datasets\n",
    "X_train, y_train = rp.datasets.bacteria(\"train\", folder=\"../data/bacteria/\")\n",
    "X_test, y_test = rp.datasets.bacteria(\"test\", folder=\"../data/bacteria/\")\n",
    "X_val, y_val = rp.datasets.bacteria(\"val\", folder=\"../data/bacteria/\")\n",
    "\n",
    "# Convert to numpy arrays for randomization and PyTorch compatibility\n",
    "X_train = np.array(X_train.spectral_data)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test.spectral_data)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.array(X_val.spectral_data)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Randomize the training data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "train_indices = np.random.permutation(len(X_train))\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 20\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a simple neural network\n",
    "class SpectraNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectraNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000, 512)  # Adjust input size to 1000\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 30)  # Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x  # Return raw logits\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SpectraNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute validation loss and accuracy\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            val_outputs = model(X_val_batch)\n",
    "            val_loss += criterion(val_outputs, y_val_batch).item()\n",
    "            y_pred_classes = torch.argmax(val_outputs, axis=1)\n",
    "            correct += (y_pred_classes == y_val_batch).sum().item()\n",
    "            total += y_val_batch.size(0)\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = X_test_tensor\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_classes = torch.argmax(y_pred, axis=1)\n",
    "    accuracy = (y_pred_classes == y_test_tensor).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from claude (CNN part)\n",
    "FCNN vs 1DCNN + FCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_spectra(spectra, noise_level=0.02):\n",
    "    noise = torch.randn_like(spectra) * noise_level\n",
    "    augmented = spectra + noise\n",
    "    return augmented\n",
    "\n",
    "class SpectraFCNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(SpectraFCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, 30)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class SpectraCNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(SpectraCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=9, padding=4),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.flatten_size = 128 * 125\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(128, 30)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model_type=\"cnn\", batch_size=50, epochs=50, learning_rate=0.001):\n",
    "    X_train, y_train = rp.datasets.bacteria(\"train\", folder=\"../data/bacteria/\")\n",
    "    X_test, y_test = rp.datasets.bacteria(\"test\", folder=\"../data/bacteria/\")\n",
    "    X_val, y_val = rp.datasets.bacteria(\"val\", folder=\"../data/bacteria/\")\n",
    "\n",
    "    X_train = np.array(X_train.spectral_data)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test.spectral_data)\n",
    "    y_test = np.array(y_test)\n",
    "    X_val = np.array(X_val.spectral_data)\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = SpectraCNN() if model_type == \"cnn\" else SpectraFCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = augment_spectra(X_batch)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                outputs = model(X_val)\n",
    "                val_loss += criterion(outputs, y_val).item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += y_val.size(0)\n",
    "                val_correct += (predicted == y_val).sum().item()\n",
    "        \n",
    "        val_accuracy = val_correct / total_val\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        print(f\"Training Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), f'best_model_{model_type}.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(torch.load(f'best_model_{model_type}.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        test_accuracy = (predicted == y_test_tensor).float().mean()\n",
    "        print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    n_classes = 30\n",
    "    class_correct = list(0. for i in range(n_classes))\n",
    "    class_total = list(0. for i in range(n_classes))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == y_test_tensor)\n",
    "        \n",
    "        for i in range(len(y_test_tensor)):\n",
    "            label = y_test_tensor[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    print(\"\\nPer-class accuracy:\")\n",
    "    for i in range(n_classes):\n",
    "        if class_total[i] > 0:\n",
    "            print(f'Accuracy of class {i}: {100 * class_correct[i] / class_total[i]:.2f}%')\n",
    "\n",
    "    return model, test_accuracy\n",
    "\n",
    "print(\"Training CNN model...\")\n",
    "cnn_model, cnn_accuracy = train_model(model_type=\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "Training Loss: 0.6917\n",
      "Validation Loss: 2.9440\n",
      "Validation Accuracy: 0.4527\n",
      "--------------------------------------------------\n",
      "Epoch [2/50]\n",
      "Training Loss: 0.3194\n",
      "Validation Loss: 3.4289\n",
      "Validation Accuracy: 0.4207\n",
      "--------------------------------------------------\n",
      "Epoch [3/50]\n",
      "Training Loss: 0.2645\n",
      "Validation Loss: 3.7003\n",
      "Validation Accuracy: 0.4170\n",
      "--------------------------------------------------\n",
      "Epoch [4/50]\n",
      "Training Loss: 0.2313\n",
      "Validation Loss: 5.0119\n",
      "Validation Accuracy: 0.3410\n",
      "--------------------------------------------------\n",
      "Epoch [5/50]\n",
      "Training Loss: 0.2072\n",
      "Validation Loss: 3.9318\n",
      "Validation Accuracy: 0.4417\n",
      "--------------------------------------------------\n",
      "Epoch [6/50]\n",
      "Training Loss: 0.1905\n",
      "Validation Loss: 4.0163\n",
      "Validation Accuracy: 0.4430\n",
      "--------------------------------------------------\n",
      "Early stopping triggered!\n",
      "Final Test Accuracy: 0.4360\n",
      "\n",
      "Per-class accuracy:\n",
      "Accuracy of class 0: 94.00%\n",
      "Accuracy of class 1: 0.00%\n",
      "Accuracy of class 2: 3.00%\n",
      "Accuracy of class 3: 100.00%\n",
      "Accuracy of class 4: 69.00%\n",
      "Accuracy of class 5: 100.00%\n",
      "Accuracy of class 6: 34.00%\n",
      "Accuracy of class 7: 45.00%\n",
      "Accuracy of class 8: 3.00%\n",
      "Accuracy of class 9: 0.00%\n",
      "Accuracy of class 10: 39.00%\n",
      "Accuracy of class 11: 5.00%\n",
      "Accuracy of class 12: 3.00%\n",
      "Accuracy of class 13: 0.00%\n",
      "Accuracy of class 14: 91.00%\n",
      "Accuracy of class 15: 85.00%\n",
      "Accuracy of class 16: 39.00%\n",
      "Accuracy of class 17: 95.00%\n",
      "Accuracy of class 18: 90.00%\n",
      "Accuracy of class 19: 86.00%\n",
      "Accuracy of class 20: 68.00%\n",
      "Accuracy of class 21: 0.00%\n",
      "Accuracy of class 22: 5.00%\n",
      "Accuracy of class 23: 85.00%\n",
      "Accuracy of class 24: 0.00%\n",
      "Accuracy of class 25: 6.00%\n",
      "Accuracy of class 26: 12.00%\n",
      "Accuracy of class 27: 99.00%\n",
      "Accuracy of class 28: 45.00%\n",
      "Accuracy of class 29: 7.00%\n"
     ]
    }
   ],
   "source": [
    "# Train FCNN model\n",
    "print(\"\\nTraining FCNN model...\")\n",
    "fcnn_model, fcnn_accuracy = train_model(model_type=\"fcnn\")\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"CNN Model Accuracy: {cnn_accuracy:.4f}\")\n",
    "print(f\"FCNN Model Accuracy: {fcnn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude again\n",
    "Enhanced data augmentation + stronger regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Initial class distribution:\n",
      "\n",
      "Train set class distribution:\n",
      "Class 0: 2000\n",
      "Class 1: 2000\n",
      "Class 2: 2000\n",
      "Class 3: 2000\n",
      "Class 4: 2000\n",
      "Class 5: 2000\n",
      "Class 6: 2000\n",
      "Class 7: 2000\n",
      "Class 8: 2000\n",
      "Class 9: 2000\n",
      "Class 10: 2000\n",
      "Class 11: 2000\n",
      "Class 12: 2000\n",
      "Class 13: 2000\n",
      "Class 14: 2000\n",
      "Class 15: 2000\n",
      "Class 16: 2000\n",
      "Class 17: 2000\n",
      "Class 18: 2000\n",
      "Class 19: 2000\n",
      "Class 20: 2000\n",
      "Class 21: 2000\n",
      "Class 22: 2000\n",
      "Class 23: 2000\n",
      "Class 24: 2000\n",
      "Class 25: 2000\n",
      "Class 26: 2000\n",
      "Class 27: 2000\n",
      "Class 28: 2000\n",
      "Class 29: 2000\n",
      "\n",
      "Validation set class distribution:\n",
      "Class 0: 100\n",
      "Class 1: 100\n",
      "Class 2: 100\n",
      "Class 3: 100\n",
      "Class 4: 100\n",
      "Class 5: 100\n",
      "Class 6: 100\n",
      "Class 7: 100\n",
      "Class 8: 100\n",
      "Class 9: 100\n",
      "Class 10: 100\n",
      "Class 11: 100\n",
      "Class 12: 100\n",
      "Class 13: 100\n",
      "Class 14: 100\n",
      "Class 15: 100\n",
      "Class 16: 100\n",
      "Class 17: 100\n",
      "Class 18: 100\n",
      "Class 19: 100\n",
      "Class 20: 100\n",
      "Class 21: 100\n",
      "Class 22: 100\n",
      "Class 23: 100\n",
      "Class 24: 100\n",
      "Class 25: 100\n",
      "Class 26: 100\n",
      "Class 27: 100\n",
      "Class 28: 100\n",
      "Class 29: 100\n",
      "\n",
      "Test set class distribution:\n",
      "Class 0: 100\n",
      "Class 1: 100\n",
      "Class 2: 100\n",
      "Class 3: 100\n",
      "Class 4: 100\n",
      "Class 5: 100\n",
      "Class 6: 100\n",
      "Class 7: 100\n",
      "Class 8: 100\n",
      "Class 9: 100\n",
      "Class 10: 100\n",
      "Class 11: 100\n",
      "Class 12: 100\n",
      "Class 13: 100\n",
      "Class 14: 100\n",
      "Class 15: 100\n",
      "Class 16: 100\n",
      "Class 17: 100\n",
      "Class 18: 100\n",
      "Class 19: 100\n",
      "Class 20: 100\n",
      "Class 21: 100\n",
      "Class 22: 100\n",
      "Class 23: 100\n",
      "Class 24: 100\n",
      "Class 25: 100\n",
      "Class 26: 100\n",
      "Class 27: 100\n",
      "Class 28: 100\n",
      "Class 29: 100\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "Starting training...\n",
      "Epoch [1/50]\n",
      "Training Loss: 1.0848\n",
      "Training Accuracy: 0.6506\n",
      "Validation Loss: 2.7829\n",
      "Validation Accuracy: 0.4210\n",
      "--------------------------------------------------\n",
      "Epoch [2/50]\n",
      "Training Loss: 0.6120\n",
      "Training Accuracy: 0.7921\n",
      "Validation Loss: 2.5944\n",
      "Validation Accuracy: 0.4500\n",
      "--------------------------------------------------\n",
      "Epoch [3/50]\n",
      "Training Loss: 0.5307\n",
      "Training Accuracy: 0.8216\n",
      "Validation Loss: 3.0127\n",
      "Validation Accuracy: 0.4413\n",
      "--------------------------------------------------\n",
      "Epoch [4/50]\n",
      "Training Loss: 0.4953\n",
      "Training Accuracy: 0.8364\n",
      "Validation Loss: 3.8250\n",
      "Validation Accuracy: 0.3723\n",
      "--------------------------------------------------\n",
      "Epoch [5/50]\n",
      "Training Loss: 0.4687\n",
      "Training Accuracy: 0.8428\n",
      "Validation Loss: 3.0897\n",
      "Validation Accuracy: 0.4493\n",
      "--------------------------------------------------\n",
      "Epoch [6/50]\n",
      "Training Loss: 0.4466\n",
      "Training Accuracy: 0.8507\n",
      "Validation Loss: 3.3941\n",
      "Validation Accuracy: 0.4423\n",
      "--------------------------------------------------\n",
      "Epoch [7/50]\n",
      "Training Loss: 0.4357\n",
      "Training Accuracy: 0.8560\n",
      "Validation Loss: 2.8070\n",
      "Validation Accuracy: 0.4710\n",
      "--------------------------------------------------\n",
      "Epoch [8/50]\n",
      "Training Loss: 0.4210\n",
      "Training Accuracy: 0.8629\n",
      "Validation Loss: 3.1360\n",
      "Validation Accuracy: 0.4580\n",
      "--------------------------------------------------\n",
      "Epoch [9/50]\n",
      "Training Loss: 0.4162\n",
      "Training Accuracy: 0.8645\n",
      "Validation Loss: 2.9022\n",
      "Validation Accuracy: 0.4690\n",
      "--------------------------------------------------\n",
      "Epoch [10/50]\n",
      "Training Loss: 0.4057\n",
      "Training Accuracy: 0.8667\n",
      "Validation Loss: 3.4894\n",
      "Validation Accuracy: 0.4367\n",
      "--------------------------------------------------\n",
      "Epoch [11/50]\n",
      "Training Loss: 0.3973\n",
      "Training Accuracy: 0.8683\n",
      "Validation Loss: 3.3185\n",
      "Validation Accuracy: 0.4410\n",
      "--------------------------------------------------\n",
      "Epoch [12/50]\n",
      "Training Loss: 0.3920\n",
      "Training Accuracy: 0.8718\n",
      "Validation Loss: 4.2551\n",
      "Validation Accuracy: 0.3907\n",
      "--------------------------------------------------\n",
      "Epoch [13/50]\n",
      "Training Loss: 0.3846\n",
      "Training Accuracy: 0.8726\n",
      "Validation Loss: 3.5458\n",
      "Validation Accuracy: 0.4360\n",
      "--------------------------------------------------\n",
      "Epoch [14/50]\n",
      "Training Loss: 0.3769\n",
      "Training Accuracy: 0.8782\n",
      "Validation Loss: 3.6782\n",
      "Validation Accuracy: 0.4143\n",
      "--------------------------------------------------\n",
      "Early stopping triggered!\n",
      "\n",
      "Evaluating final model...\n",
      "Final Test Accuracy: 0.4700\n"
     ]
    }
   ],
   "source": [
    "# Enhanced data augmentation function\n",
    "def augment_spectra(spectra, noise_level=0.05):\n",
    "    noise = torch.randn_like(spectra) * noise_level\n",
    "    scale = 1.0 + (torch.rand_like(spectra) * 0.1 - 0.05)\n",
    "    augmented = spectra * scale + noise\n",
    "    return augmented\n",
    "\n",
    "# Improved neural network with stronger regularization\n",
    "class SpectraNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(SpectraNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, 30)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train = rp.datasets.bacteria(\"train\", folder=\"../data/bacteria/\")\n",
    "X_test, y_test = rp.datasets.bacteria(\"test\", folder=\"../data/bacteria/\")\n",
    "X_val, y_val = rp.datasets.bacteria(\"val\", folder=\"../data/bacteria/\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train.spectral_data)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test.spectral_data)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.array(X_val.spectral_data)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Print initial class distribution\n",
    "print(\"\\nInitial class distribution:\")\n",
    "for split_name, y_data in [('Train', y_train), ('Validation', y_val), ('Test', y_test)]:\n",
    "    unique, counts = np.unique(y_data, return_counts=True)\n",
    "    print(f\"\\n{split_name} set class distribution:\")\n",
    "    for class_idx, count in zip(unique, counts):\n",
    "        print(f\"Class {class_idx}: {count}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Reduced batch size\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Calculate class weights for balanced loss\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "weights = 1.0 / torch.tensor(counts, dtype=torch.float)\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "# Initialize model, loss function, optimizer, and scheduler\n",
    "print(\"\\nInitializing model...\")\n",
    "model = SpectraNN()\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Added L2 regularization\n",
    "epochs = 50\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 7  # Increased patience\n",
    "patience_counter = 0\n",
    "best_accuracy = 0.0\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Apply data augmentation\n",
    "        X_batch = augment_spectra(X_batch)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            outputs = model(X_val)\n",
    "            val_loss += criterion(outputs, y_val).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += (predicted == y_val).sum().item()\n",
    "    \n",
    "    train_accuracy = train_correct / train_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"Training Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "print(\"\\nEvaluating final model...\")\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(test_outputs.data, 1)\n",
    "    test_accuracy = (predicted == y_test_tensor).float().mean()\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude againnn\n",
    "Residual connections model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "Starting training...\n",
      "Epoch [1/100]\n",
      "Training Loss: 1.0214\n",
      "Training Accuracy: 0.6675\n",
      "Validation Loss: 4.3725\n",
      "Validation Accuracy: 0.0810\n",
      "--------------------------------------------------\n",
      "Epoch [2/100]\n",
      "Training Loss: 0.6525\n",
      "Training Accuracy: 0.7845\n",
      "Validation Loss: 4.3053\n",
      "Validation Accuracy: 0.1347\n",
      "--------------------------------------------------\n",
      "Epoch [3/100]\n",
      "Training Loss: 0.5731\n",
      "Training Accuracy: 0.8096\n",
      "Validation Loss: 4.3579\n",
      "Validation Accuracy: 0.0770\n",
      "--------------------------------------------------\n",
      "Epoch [4/100]\n",
      "Training Loss: 0.5293\n",
      "Training Accuracy: 0.8279\n",
      "Validation Loss: 4.2040\n",
      "Validation Accuracy: 0.1593\n",
      "--------------------------------------------------\n",
      "Epoch [5/100]\n",
      "Training Loss: 0.4963\n",
      "Training Accuracy: 0.8388\n",
      "Validation Loss: 4.6408\n",
      "Validation Accuracy: 0.1007\n",
      "--------------------------------------------------\n",
      "Epoch [6/100]\n",
      "Training Loss: 0.4805\n",
      "Training Accuracy: 0.8427\n",
      "Validation Loss: 4.4679\n",
      "Validation Accuracy: 0.0943\n",
      "--------------------------------------------------\n",
      "Epoch [7/100]\n",
      "Training Loss: 0.4520\n",
      "Training Accuracy: 0.8543\n",
      "Validation Loss: 4.7838\n",
      "Validation Accuracy: 0.1117\n",
      "--------------------------------------------------\n",
      "Epoch [8/100]\n",
      "Training Loss: 0.4489\n",
      "Training Accuracy: 0.8558\n",
      "Validation Loss: 4.3588\n",
      "Validation Accuracy: 0.1540\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Enhanced data augmentation with more variations\n",
    "def augment_spectra(spectra, noise_level=0.03):\n",
    "    # Add random noise\n",
    "    noise = torch.randn_like(spectra) * noise_level\n",
    "    # Random scaling\n",
    "    scale = 1.0 + (torch.rand_like(spectra) * 0.1 - 0.05)\n",
    "    # Random shift\n",
    "    shift = torch.rand_like(spectra) * 0.02 - 0.01\n",
    "    augmented = spectra * scale + noise + shift\n",
    "    return augmented\n",
    "\n",
    "# Modified network architecture with residual connections\n",
    "class SpectraNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.4):\n",
    "        super(SpectraNN, self).__init__()\n",
    "        self.input_bn = nn.BatchNorm1d(1000)\n",
    "        \n",
    "        # First block\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Second block with residual connection\n",
    "        self.block2_1 = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512)\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 30)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_bn(x)\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2_1(x1)\n",
    "        x = x1 + x2  # Residual connection\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train = rp.datasets.bacteria(\"train\", folder=\"../data/bacteria/\")\n",
    "X_test, y_test = rp.datasets.bacteria(\"test\", folder=\"../data/bacteria/\")\n",
    "X_val, y_val = rp.datasets.bacteria(\"val\", folder=\"../data/bacteria/\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train.spectral_data)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test.spectral_data)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.array(X_val.spectral_data)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders with smaller batch size\n",
    "batch_size = 10\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model, loss function, optimizer, and scheduler\n",
    "print(\"\\nInitializing model...\")\n",
    "model = SpectraNN()\n",
    "\n",
    "# Calculate class weights for balanced loss\n",
    "weights = torch.ones(30) # Equal weights since data is balanced\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "epochs = 100  # Increased epochs\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 10  # Increased patience\n",
    "patience_counter = 0\n",
    "best_accuracy = 0.0\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Apply data augmentation\n",
    "        X_batch = augment_spectra(X_batch)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            outputs = model(X_val)\n",
    "            val_loss += criterion(outputs, y_val).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += (predicted == y_val).sum().item()\n",
    "    \n",
    "    train_accuracy = train_correct / train_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"Training Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "        }, 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "print(\"\\nEvaluating final model...\")\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Evaluate with test-time augmentation\n",
    "def test_time_augment(model, X, num_augments=5):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        # Original prediction\n",
    "        outputs = model(X)\n",
    "        predictions.append(outputs)\n",
    "        \n",
    "        # Augmented predictions\n",
    "        for _ in range(num_augments - 1):\n",
    "            aug_X = augment_spectra(X)\n",
    "            outputs = model(aug_X)\n",
    "            predictions.append(outputs)\n",
    "    \n",
    "    # Average predictions\n",
    "    return torch.stack(predictions).mean(0)\n",
    "\n",
    "# Final evaluation with test-time augmentation\n",
    "outputs = test_time_augment(model, X_test_tensor)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "test_accuracy = (predicted == y_test_tensor).float().mean()\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print per-class accuracy\n",
    "print(\"\\nPer-class test accuracy:\")\n",
    "n_classes = 30\n",
    "class_correct = list(0. for i in range(n_classes))\n",
    "class_total = list(0. for i in range(n_classes))\n",
    "\n",
    "with torch.no_grad():\n",
    "    c = (predicted == y_test_tensor)\n",
    "    for i in range(len(y_test_tensor)):\n",
    "        label = y_test_tensor[i]\n",
    "        class_correct[label] += c[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "for i in range(n_classes):\n",
    "    if class_total[i] > 0:\n",
    "        print(f'Accuracy of class {i}: {100 * class_correct[i] / class_total[i]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude again againnnnnn\n",
    "Self-attention + focal loss (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Splitting data...\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "Starting training...\n",
      "Epoch: 1\n",
      "Train Loss: 0.5521 | Train Acc: 0.7592\n",
      "Val Loss: 0.1264 | Val Acc: 0.9042\n",
      "--------------------------------------------------\n",
      "Epoch: 2\n",
      "Train Loss: 0.2202 | Train Acc: 0.8583\n",
      "Val Loss: 0.1099 | Val Acc: 0.9171\n",
      "--------------------------------------------------\n",
      "Epoch: 3\n",
      "Train Loss: 0.1802 | Train Acc: 0.8804\n",
      "Val Loss: 0.1029 | Val Acc: 0.9194\n",
      "--------------------------------------------------\n",
      "Epoch: 4\n",
      "Train Loss: 0.1622 | Train Acc: 0.8898\n",
      "Val Loss: 0.0959 | Val Acc: 0.9230\n",
      "--------------------------------------------------\n",
      "Epoch: 5\n",
      "Train Loss: 0.1468 | Train Acc: 0.8989\n",
      "Val Loss: 0.0869 | Val Acc: 0.9287\n",
      "--------------------------------------------------\n",
      "Epoch: 6\n",
      "Train Loss: 0.1385 | Train Acc: 0.9020\n",
      "Val Loss: 0.0912 | Val Acc: 0.9270\n",
      "--------------------------------------------------\n",
      "Epoch: 7\n",
      "Train Loss: 0.1260 | Train Acc: 0.9089\n",
      "Val Loss: 0.0934 | Val Acc: 0.9274\n",
      "--------------------------------------------------\n",
      "Epoch: 8\n",
      "Train Loss: 0.1194 | Train Acc: 0.9106\n",
      "Val Loss: 0.0866 | Val Acc: 0.9300\n",
      "--------------------------------------------------\n",
      "Epoch: 9\n",
      "Train Loss: 0.1150 | Train Acc: 0.9142\n",
      "Val Loss: 0.0860 | Val Acc: 0.9311\n",
      "--------------------------------------------------\n",
      "Epoch: 10\n",
      "Train Loss: 0.1091 | Train Acc: 0.9178\n",
      "Val Loss: 0.0831 | Val Acc: 0.9357\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tb/4n3fh0qx0q734bc626dps61r0000gn/T/ipykernel_3133/1742144458.py:237: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy: 0.4000\n",
      "\n",
      "Per-class accuracy:\n",
      "Accuracy of class 0: 92.00%\n",
      "Accuracy of class 1: 0.00%\n",
      "Accuracy of class 2: 2.00%\n",
      "Accuracy of class 3: 79.00%\n",
      "Accuracy of class 4: 17.00%\n",
      "Accuracy of class 5: 100.00%\n",
      "Accuracy of class 6: 55.00%\n",
      "Accuracy of class 7: 35.00%\n",
      "Accuracy of class 8: 0.00%\n",
      "Accuracy of class 9: 0.00%\n",
      "Accuracy of class 10: 14.00%\n",
      "Accuracy of class 11: 0.00%\n",
      "Accuracy of class 12: 5.00%\n",
      "Accuracy of class 13: 1.00%\n",
      "Accuracy of class 14: 92.00%\n",
      "Accuracy of class 15: 98.00%\n",
      "Accuracy of class 16: 0.00%\n",
      "Accuracy of class 17: 98.00%\n",
      "Accuracy of class 18: 95.00%\n",
      "Accuracy of class 19: 79.00%\n",
      "Accuracy of class 20: 98.00%\n",
      "Accuracy of class 21: 2.00%\n",
      "Accuracy of class 22: 0.00%\n",
      "Accuracy of class 23: 83.00%\n",
      "Accuracy of class 24: 0.00%\n",
      "Accuracy of class 25: 17.00%\n",
      "Accuracy of class 26: 19.00%\n",
      "Accuracy of class 27: 93.00%\n",
      "Accuracy of class 28: 20.00%\n",
      "Accuracy of class 29: 6.00%\n"
     ]
    }
   ],
   "source": [
    "# Focal Loss implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Self-attention module\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(in_dim, in_dim)\n",
    "        self.key = nn.Linear(in_dim, in_dim)\n",
    "        self.value = nn.Linear(in_dim, in_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(x.size(-1))\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attention, v)\n",
    "        return out\n",
    "\n",
    "# Enhanced network architecture\n",
    "class SpectraNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(SpectraNN, self).__init__()\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_bn = nn.BatchNorm1d(1000)\n",
    "        self.input_attention = SelfAttention(1000)\n",
    "        \n",
    "        # Deeper architecture with residual connections\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.attention = SelfAttention(512)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(128, 30)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_bn(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.input_attention(x)\n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.attention(x1.unsqueeze(1)).squeeze(1)\n",
    "        x = x1 + x2  # Residual connection\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Advanced data augmentation\n",
    "def augment_spectra(spectra, noise_level=0.02):\n",
    "    augmented = spectra.clone()\n",
    "    \n",
    "    # Random noise\n",
    "    noise = torch.randn_like(spectra) * noise_level\n",
    "    augmented += noise\n",
    "    \n",
    "    # Random scaling\n",
    "    scale = 1.0 + (torch.rand_like(spectra) * 0.1 - 0.05)\n",
    "    augmented *= scale\n",
    "    \n",
    "    # Random shifts\n",
    "    shift = (torch.rand_like(spectra) * 0.02 - 0.01)\n",
    "    augmented += shift\n",
    "    \n",
    "    # Random smoothing (randomly applied)\n",
    "    if torch.rand(1) > 0.5:\n",
    "        kernel_size = 3\n",
    "        pad_size = kernel_size // 2\n",
    "        padded = torch.nn.functional.pad(augmented.unsqueeze(1), (pad_size, pad_size), mode='reflect')\n",
    "        augmented = torch.nn.functional.avg_pool1d(padded, kernel_size=kernel_size, stride=1).squeeze(1)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train = rp.datasets.bacteria(\"train\", folder=\"../data/bacteria/\")\n",
    "X_test, y_test = rp.datasets.bacteria(\"test\", folder=\"../data/bacteria/\")\n",
    "X_val, y_val = rp.datasets.bacteria(\"val\", folder=\"../data/bacteria/\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train.spectral_data)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test.spectral_data)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.array(X_val.spectral_data)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model and training components\n",
    "print(\"\\nInitializing model...\")\n",
    "model = SpectraNN()\n",
    "criterion = FocalLoss(gamma=2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "epochs = 10\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        # Multiple augmentations per batch\n",
    "        X_aug = torch.cat([augment_spectra(X_batch) for _ in range(2)])\n",
    "        y_aug = torch.cat([y_batch for _ in range(2)])\n",
    "        \n",
    "        outputs = model(X_aug)\n",
    "        loss = criterion(outputs, y_aug)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += y_aug.size(0)\n",
    "        train_correct += predicted.eq(y_aug).sum().item()\n",
    "        \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            outputs = model(X_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += predicted.eq(y_val).sum().item()\n",
    "    \n",
    "    # Print metrics\n",
    "    train_accuracy = train_correct / train_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    print(f'Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_accuracy:.4f}')\n",
    "    print(f'Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_accuracy:.4f}')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "# Test evaluation with ensemble predictions\n",
    "print(\"\\nEvaluating final model...\")\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def ensemble_predict(model, X, n_augmentations=5):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        # Original prediction\n",
    "        pred = model(X)\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        # Augmented predictions\n",
    "        for _ in range(n_augmentations-1):\n",
    "            aug_X = augment_spectra(X)\n",
    "            pred = model(aug_X)\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    return torch.stack(predictions).mean(0)\n",
    "\n",
    "# Final evaluation\n",
    "outputs = ensemble_predict(model, X_test_tensor)\n",
    "_, predicted = outputs.max(1)\n",
    "accuracy = predicted.eq(y_test_tensor).float().mean()\n",
    "print(f'\\nFinal Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Per-class accuracy\n",
    "class_correct = list(0. for i in range(30))\n",
    "class_total = list(0. for i in range(30))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(y_test_tensor)):\n",
    "        label = y_test_tensor[i]\n",
    "        class_correct[label] += (predicted[i] == label).item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "print('\\nPer-class accuracy:')\n",
    "for i in range(30):\n",
    "    if class_total[i] > 0:\n",
    "        print(f'Accuracy of class {i}: {100 * class_correct[i] / class_total[i]:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
