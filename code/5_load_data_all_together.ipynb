{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILS Data Columns: Index(['labcode', 'substrate', 'laser', 'method', 'sample', 'type', 'conc',\n",
      "       'batch', 'replica', '400',\n",
      "       ...\n",
      "       '1972', '1975', '1978', '1981', '1984', '1987', '1990', '1993', '1996',\n",
      "       '1999'],\n",
      "      dtype='object', length=543)\n",
      "ILS Data Shape: (3516, 543)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ramanspy as rp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load ILSdata.csv\n",
    "file_path = \"../data/dataset/ILSdata.csv\"\n",
    "ils_data = pd.read_csv(file_path)\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"ILS Data Columns:\", ils_data.columns)\n",
    "print(\"ILS Data Shape:\", ils_data.shape)\n",
    "\n",
    "ils_target = data['substrate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of spectra_data: (3516, 534)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ILS dataset\n",
    "file_path = \"../data/dataset/ILSdata.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separate metadata and spectra\n",
    "metadata_columns = ['labcode', 'substrate', 'laser', 'method', 'sample', 'type', 'conc', 'batch', 'replica']\n",
    "spectra_data = data.drop(columns=metadata_columns)\n",
    "\n",
    "# Print the shape of spectra_data\n",
    "print(\"Shape of spectra_data:\", spectra_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows and columns of spectra_data:\n",
      "       400      403      406      409      412      415      418      421  \\\n",
      "0  66533.0  66322.0  66170.0  66073.0  66129.0  66154.0  65938.0  65752.0   \n",
      "1  95228.0  95066.0  94977.0  94967.0  94976.0  94761.0  94734.0  94846.0   \n",
      "2  80044.0  80204.0  80182.0  79995.0  79751.0  79644.0  79693.0  79643.0   \n",
      "3  77645.0  77972.0  77634.0  76632.0  75875.0  75897.0  75800.0  75655.0   \n",
      "4  85099.0  84810.0  85071.0  86000.0  86656.0  85530.0  84496.0  84636.0   \n",
      "\n",
      "       424      427  \n",
      "0  65636.0  65475.0  \n",
      "1  94574.0  94260.0  \n",
      "2  79324.0  79090.0  \n",
      "3  75470.0  75443.0  \n",
      "4  84639.0  84617.0  \n",
      "Number of missing values in spectra_data: 27180\n",
      "Data types of spectra_data columns:\n",
      "400     float64\n",
      "403     float64\n",
      "406     float64\n",
      "409     float64\n",
      "412     float64\n",
      "         ...   \n",
      "1987    float64\n",
      "1990    float64\n",
      "1993    float64\n",
      "1996    float64\n",
      "1999    float64\n",
      "Length: 534, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows and columns\n",
    "print(\"First few rows and columns of spectra_data:\")\n",
    "print(spectra_data.iloc[:5, :10])  # First 5 rows and first 10 columns\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Number of missing values in spectra_data:\", spectra_data.isna().sum().sum())\n",
    "\n",
    "# Check the data types\n",
    "print(\"Data types of spectra_data columns:\")\n",
    "print(spectra_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted wavelengths for ILS dataset: 400      400.0\n",
      "403      403.0\n",
      "406      406.0\n",
      "409      409.0\n",
      "412      412.0\n",
      "         ...  \n",
      "1987    1987.0\n",
      "1990    1990.0\n",
      "1993    1993.0\n",
      "1996    1996.0\n",
      "1999    1999.0\n",
      "Length: 534, dtype: float64\n",
      "Minimum wavelength: 400.0\n",
      "Maximum wavelength: 1999.0\n",
      "Shape of spectra_ils: (3516, 534)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Extract numeric part from column names\n",
    "original_wavelengths_ils = spectra_data.columns.to_series().apply(lambda x: float(re.findall(r'\\d+', x)[0]))\n",
    "\n",
    "# Print the extracted wavelengths\n",
    "print(\"Extracted wavelengths for ILS dataset:\", original_wavelengths_ils)\n",
    "print(\"Minimum wavelength:\", original_wavelengths_ils.min())\n",
    "print(\"Maximum wavelength:\", original_wavelengths_ils.max())\n",
    "\n",
    "# Convert spectra to a NumPy array\n",
    "spectra_ils = spectra_data.to_numpy()\n",
    "\n",
    "# Print the shape of the NumPy array\n",
    "print(\"Shape of spectra_ils:\", spectra_ils.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacteria Data Shape: (66000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Load training and testing datasets\n",
    "X_train, y_train = rp.datasets.bacteria(\"train\", folder=\"../data/bacteria/\")\n",
    "X_test, y_test = rp.datasets.bacteria(\"test\", folder=\"../data/bacteria/\")\n",
    "X_val, y_val = rp.datasets.bacteria(\"val\", folder=\"../data/bacteria/\")\n",
    "\n",
    "# Convert to numpy arrays for randomization and PyTorch compatibility\n",
    "X_train_bacteria = np.array(X_train.spectral_data)\n",
    "y_train_bacteria = np.array(y_train)\n",
    "X_test_bacteria = np.array(X_test.spectral_data)\n",
    "y_test_bacteria = np.array(y_test)\n",
    "X_val_bacteria = np.array(X_val.spectral_data)\n",
    "y_val_bacteria = np.array(y_val)\n",
    "\n",
    "# Combine bacteria datasets\n",
    "X_bacteria = np.vstack([X_train_bacteria, X_test_bacteria, X_val_bacteria])\n",
    "\n",
    "# Check the shape\n",
    "print(\"Bacteria Data Shape:\", X_bacteria.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Type of spectra_wheat:\", type(spectra_wheat))\n",
    "print(\"Available attributes and methods:\", dir(spectra_wheat))\n",
    "\n",
    "spectra_wheat_data = spectra_wheat.spectral_data\n",
    "\n",
    "print(\"Shape of spectra_wheat_data:\", spectra_wheat_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "df_wheats = rp.datasets.wheat_lines(file=\"../wheats/Data.mat\")\n",
    "spectra, labels, label_names = df_wheats\n",
    "\n",
    "\n",
    "# Encode the labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Convert the encoded labels to one-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "labels_one_hot = to_categorical(labels_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0.])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_one_hot[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILS Data Shape: (3516, 534)\n",
      "Bacteria Data Shape: (66000, 1000)\n",
      "Wheat Data Shape: (53134, 1748)\n"
     ]
    }
   ],
   "source": [
    "print(\"ILS Data Shape:\", spectra_ils.shape)\n",
    "print(\"Bacteria Data Shape:\", X_bacteria.shape)\n",
    "print(\"Wheat Data Shape:\", spectra_wheat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacteria Data Shape: (66000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Bacteria Data Shape:\", X_bacteria.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define metadata_columns (as you provided)\n",
    "metadata_columns = ['labcode', 'substrate', 'laser', 'method', 'sample', 'type', 'conc', 'batch', 'replica']\n",
    "\n",
    "# Extract spectral data from ILS dataset\n",
    "spectra_ils = data.drop(columns=metadata_columns).to_numpy()\n",
    "\n",
    "# Extract spectral data from wheat dataset\n",
    "spectra_wheat = spectra_wheat.spectral_data\n",
    "\n",
    "# Combine all spectral data\n",
    "X_combined = np.vstack([spectra_ils, X_bacteria, spectra_wheat])\n",
    "\n",
    "# Print the combined shape\n",
    "print(\"Combined Spectral Data Shape:\", X_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the wavelength values are stored in a separate array or file\n",
    "original_wavelengths_bacteria = np.linspace(250, 2000, X_bacteria.shape[1])  # Example: 250 to 2000 cm⁻¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacteria Wavenumbers Shape: (1000,)\n",
      "Bacteria Wavenumbers: [1792.4  1791.2  1789.9  1788.6  1787.3  1786.   1784.8  1783.5  1782.2\n",
      " 1780.9  1779.6  1778.3  1777.1  1775.8  1774.5  1773.2  1771.9  1770.6\n",
      " 1769.4  1768.1  1766.8  1765.5  1764.2  1762.9  1761.7  1760.4  1759.1\n",
      " 1757.8  1756.5  1755.2  1753.9  1752.6  1751.3  1750.1  1748.8  1747.5\n",
      " 1746.2  1744.9  1743.6  1742.3  1741.   1739.8  1738.5  1737.2  1735.9\n",
      " 1734.6  1733.3  1732.   1730.7  1729.4  1728.1  1726.8  1725.5  1724.3\n",
      " 1723.   1721.7  1720.4  1719.1  1717.8  1716.5  1715.2  1713.9  1712.6\n",
      " 1711.3  1710.   1708.7  1707.4  1706.1  1704.8  1703.5  1702.2  1701.\n",
      " 1699.7  1698.3  1697.   1695.8  1694.5  1693.2  1691.9  1690.6  1689.3\n",
      " 1688.   1686.7  1685.4  1684.1  1682.8  1681.5  1680.2  1678.9  1677.5\n",
      " 1676.2  1675.   1673.7  1672.3  1671.   1669.7  1668.4  1667.1  1665.8\n",
      " 1664.5  1663.2  1661.9  1660.6  1659.3  1658.   1656.7  1655.4  1654.1\n",
      " 1652.8  1651.5  1650.2  1648.9  1647.5  1646.2  1644.9  1643.6  1642.3\n",
      " 1641.   1639.7  1638.4  1637.1  1635.8  1634.5  1633.2  1631.8  1630.5\n",
      " 1629.2  1627.9  1626.6  1625.3  1624.   1622.7  1621.4  1620.   1618.7\n",
      " 1617.4  1616.1  1614.8  1613.5  1612.2  1610.8  1609.5  1608.2  1606.9\n",
      " 1605.6  1604.3  1603.   1601.6  1600.3  1599.   1597.7  1596.4  1595.1\n",
      " 1593.7  1592.4  1591.1  1589.8  1588.5  1587.2  1585.8  1584.5  1583.2\n",
      " 1581.9  1580.6  1579.2  1577.9  1576.6  1575.3  1574.   1572.6  1571.3\n",
      " 1570.   1568.7  1567.3  1566.   1564.7  1563.4  1562.   1560.7  1559.4\n",
      " 1558.1  1556.8  1555.4  1554.1  1552.8  1551.5  1550.1  1548.8  1547.5\n",
      " 1546.2  1544.8  1543.5  1542.2  1540.8  1539.5  1538.2  1536.9  1535.5\n",
      " 1534.2  1532.9  1531.5  1530.2  1528.9  1527.5  1526.2  1524.9  1523.6\n",
      " 1522.2  1520.9  1519.6  1518.2  1516.9  1515.6  1514.2  1512.9  1511.6\n",
      " 1510.2  1508.9  1507.6  1506.2  1504.9  1503.6  1502.2  1500.9  1499.6\n",
      " 1498.2  1496.9  1495.6  1494.2  1492.9  1491.5  1490.2  1488.9  1487.5\n",
      " 1486.2  1484.9  1483.5  1482.2  1480.8  1479.5  1478.2  1476.8  1475.5\n",
      " 1474.1  1472.8  1471.5  1470.1  1468.8  1467.4  1466.1  1464.8  1463.4\n",
      " 1462.1  1460.7  1459.4  1458.   1456.7  1455.3  1454.   1452.7  1451.3\n",
      " 1450.   1448.6  1447.3  1445.9  1444.6  1443.2  1441.9  1440.6  1439.2\n",
      " 1437.9  1436.5  1435.2  1433.8  1432.5  1431.1  1429.8  1428.4  1427.1\n",
      " 1425.7  1424.4  1423.   1421.7  1420.3  1419.   1417.6  1416.3  1414.9\n",
      " 1413.6  1412.2  1410.9  1409.5  1408.2  1406.8  1405.5  1404.1  1402.8\n",
      " 1401.4  1400.   1398.7  1397.3  1396.   1394.6  1393.3  1391.9  1390.5\n",
      " 1389.2  1387.8  1386.5  1385.1  1383.8  1382.4  1381.   1379.7  1378.3\n",
      " 1377.   1375.6  1374.2  1372.9  1371.5  1370.2  1368.8  1367.5  1366.1\n",
      " 1364.7  1363.4  1362.   1360.7  1359.3  1357.9  1356.6  1355.2  1353.8\n",
      " 1352.5  1351.1  1349.7  1348.4  1347.   1345.7  1344.3  1342.9  1341.5\n",
      " 1340.2  1338.8  1337.5  1336.1  1334.7  1333.3  1332.   1330.6  1329.2\n",
      " 1327.9  1326.5  1325.1  1323.8  1322.4  1321.   1319.7  1318.3  1316.9\n",
      " 1315.5  1314.2  1312.8  1311.4  1310.1  1308.7  1307.3  1306.   1304.6\n",
      " 1303.2  1301.8  1300.5  1299.1  1297.7  1296.3  1295.   1293.6  1292.2\n",
      " 1290.8  1289.5  1288.1  1286.7  1285.3  1284.   1282.6  1281.2  1279.8\n",
      " 1278.4  1277.1  1275.7  1274.3  1272.9  1271.5  1270.2  1268.8  1267.4\n",
      " 1266.   1264.7  1263.3  1261.9  1260.5  1259.1  1257.7  1256.4  1255.\n",
      " 1253.6  1252.2  1250.8  1249.5  1248.1  1246.7  1245.3  1243.9  1242.5\n",
      " 1241.1  1239.8  1238.4  1237.   1235.6  1234.2  1232.8  1231.4  1230.\n",
      " 1228.7  1227.3  1225.9  1224.5  1223.1  1221.7  1220.3  1218.9  1217.5\n",
      " 1216.2  1214.8  1213.4  1212.   1210.6  1209.2  1207.8  1206.4  1205.\n",
      " 1203.6  1202.2  1200.8  1199.5  1198.1  1196.7  1195.3  1193.9  1192.5\n",
      " 1191.1  1189.7  1188.3  1186.9  1185.5  1184.1  1182.7  1181.3  1179.9\n",
      " 1178.5  1177.1  1175.7  1174.3  1172.9  1171.5  1170.1  1168.7  1167.3\n",
      " 1165.9  1164.5  1163.1  1161.7  1160.3  1158.9  1157.5  1156.1  1154.7\n",
      " 1153.3  1151.9  1150.5  1149.1  1147.7  1146.3  1144.9  1143.5  1142.1\n",
      " 1140.7  1139.3  1137.9  1136.5  1135.1  1133.7  1132.3  1130.9  1129.5\n",
      " 1128.   1126.6  1125.2  1123.8  1122.4  1121.   1119.6  1118.2  1116.8\n",
      " 1115.4  1114.   1112.5  1111.1  1109.7  1108.3  1106.9  1105.5  1104.1\n",
      " 1102.7  1101.2  1099.8  1098.4  1097.   1095.6  1094.2  1092.8  1091.4\n",
      " 1089.9  1088.5  1087.1  1085.7  1084.3  1082.9  1081.5  1080.   1078.6\n",
      " 1077.2  1075.8  1074.4  1073.   1071.5  1070.1  1068.7  1067.3  1065.8\n",
      " 1064.4  1063.   1061.6  1060.2  1058.8  1057.3  1055.9  1054.5  1053.1\n",
      " 1051.6  1050.2  1048.8  1047.4  1046.   1044.5  1043.1  1041.7  1040.3\n",
      " 1038.8  1037.4  1036.   1034.6  1033.1  1031.7  1030.3  1028.8  1027.4\n",
      " 1026.   1024.6  1023.1  1021.7  1020.3  1018.9  1017.4  1016.   1014.6\n",
      " 1013.1  1011.7  1010.3  1008.9  1007.4  1006.   1004.6  1003.1  1001.7\n",
      " 1000.3   998.83  997.39  995.96  994.53  993.09  991.66  990.23  988.79\n",
      "  987.35  985.92  984.48  983.05  981.61  980.18  978.74  977.3   975.87\n",
      "  974.43  972.99  971.55  970.12  968.68  967.24  965.8   964.36  962.92\n",
      "  961.48  960.04  958.6   957.16  955.72  954.27  952.83  951.39  949.95\n",
      "  948.51  947.06  945.62  944.18  942.73  941.29  939.85  938.4   936.96\n",
      "  935.51  934.07  932.62  931.17  929.73  928.28  926.83  925.39  923.94\n",
      "  922.49  921.04  919.6   918.15  916.7   915.25  913.8   912.35  910.9\n",
      "  909.45  908.    906.55  905.1   903.65  902.2   900.74  899.29  897.84\n",
      "  896.38  894.93  893.48  892.02  890.57  889.12  887.66  886.21  884.75\n",
      "  883.29  881.84  880.38  878.93  877.47  876.01  874.56  873.1   871.64\n",
      "  870.18  868.72  867.27  865.81  864.35  862.89  861.43  859.97  858.51\n",
      "  857.05  855.59  854.12  852.66  851.2   849.74  848.28  846.82  845.35\n",
      "  843.89  842.42  840.96  839.5   838.03  836.57  835.1   833.64  832.17\n",
      "  830.71  829.24  827.77  826.3   824.84  823.37  821.9   820.43  818.97\n",
      "  817.5   816.03  814.56  813.09  811.62  810.15  808.68  807.21  805.74\n",
      "  804.27  802.8   801.32  799.85  798.38  796.91  795.43  793.96  792.49\n",
      "  791.01  789.54  788.07  786.59  785.11  783.64  782.16  780.69  779.21\n",
      "  777.73  776.26  774.78  773.3   771.83  770.35  768.87  767.39  765.91\n",
      "  764.43  762.95  761.47  759.99  758.51  757.03  755.55  754.07  752.59\n",
      "  751.11  749.62  748.14  746.66  745.18  743.69  742.21  740.72  739.24\n",
      "  737.76  736.27  734.79  733.3   731.82  730.33  728.84  727.36  725.87\n",
      "  724.38  722.89  721.41  719.92  718.43  716.94  715.45  713.96  712.47\n",
      "  710.98  709.49  708.    706.51  705.02  703.53  702.04  700.54  699.05\n",
      "  697.56  696.07  694.57  693.08  691.59  690.09  688.6   687.1   685.61\n",
      "  684.11  682.62  681.12  679.62  678.13  676.63  675.13  673.64  672.14\n",
      "  670.64  669.14  667.64  666.14  664.64  663.15  661.65  660.15  658.65\n",
      "  657.14  655.64  654.14  652.64  651.14  649.64  648.13  646.63  645.13\n",
      "  643.62  642.12  640.62  639.11  637.61  636.1   634.6   633.09  631.59\n",
      "  630.08  628.57  627.07  625.56  624.05  622.54  621.03  619.53  618.02\n",
      "  616.51  615.    613.49  611.98  610.47  608.96  607.45  605.94  604.43\n",
      "  602.91  601.4   599.89  598.38  596.86  595.35  593.84  592.32  590.81\n",
      "  589.29  587.78  586.26  584.75  583.23  581.72  580.2   578.68  577.17\n",
      "  575.65  574.13  572.62  571.1   569.58  568.06  566.54  565.02  563.5\n",
      "  561.98  560.46  558.94  557.42  555.9   554.38  552.85  551.33  549.81\n",
      "  548.29  546.76  545.24  543.72  542.19  540.67  539.14  537.62  536.09\n",
      "  534.57  533.04  531.52  529.99  528.46  526.93  525.41  523.88  522.35\n",
      "  520.82  519.29  517.77  516.24  514.71  513.18  511.65  510.12  508.58\n",
      "  507.05  505.52  503.99  502.46  500.93  499.39  497.86  496.33  494.79\n",
      "  493.26  491.72  490.19  488.65  487.12  485.58  484.05  482.51  480.98\n",
      "  479.44  477.9   476.36  474.83  473.29  471.75  470.21  468.67  467.13\n",
      "  465.59  464.05  462.51  460.97  459.43  457.89  456.35  454.81  453.26\n",
      "  451.72  450.18  448.64  447.09  445.55  444.    442.46  440.92  439.37\n",
      "  437.82  436.28  434.73  433.19  431.64  430.09  428.55  427.    425.45\n",
      "  423.9   422.35  420.81  419.25  417.71  416.16  414.61  413.06  411.51\n",
      "  409.95  408.4   406.85  405.3   403.75  402.19  400.64  399.09  397.53\n",
      "  395.98  394.43  392.87  391.32  389.76  388.21  386.65  385.09  383.54\n",
      "  381.98]\n"
     ]
    }
   ],
   "source": [
    "# Load wavenumbers for the bacteria dataset\n",
    "original_wavelengths_bacteria = np.load(\"../data/bacteria/wavenumbers.npy\")\n",
    "\n",
    "# Check the shape and values\n",
    "print(\"Bacteria Wavenumbers Shape:\", original_wavelengths_bacteria.shape)\n",
    "print(\"Bacteria Wavenumbers:\", original_wavelengths_bacteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract numeric part from column names\n",
    "original_wavelengths_ils = spectra_data.columns.to_series().apply(lambda x: float(re.findall(r'\\d+', x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_wavelengths_ils = spectra_data.columns.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_wavelengths_ils = spectra_data.columns.to_series().apply(lambda x: float(re.findall(r'\\d+', x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate original_wavelengths_wheat based on the number of columns in spectra_wheat_data\n",
    "num_wavelengths = spectra_wheat_data.shape[1]\n",
    "original_wavelengths_wheat = np.linspace(250, 2000, num_wavelengths)  # Example range for Raman spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original_wavelengths_wheat: (1748,)\n",
      "Shape of spectra_wheat_data: (53134, 1748)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of original_wavelengths_wheat:\", original_wavelengths_wheat.shape)\n",
    "print(\"Shape of spectra_wheat_data:\", spectra_wheat_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized Wheat Data Shape: (53134, 1748)\n"
     ]
    }
   ],
   "source": [
    "# Resize the wheat dataset\n",
    "spectra_wheat_resized = resize_spectra(spectra_wheat_data, original_wavelengths_wheat, target_size)\n",
    "\n",
    "# Print the resized shape\n",
    "print(\"Resized Wheat Data Shape:\", spectra_wheat_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized ILS Data Shape: (3516, 1748)\n",
      "Resized Bacteria Data Shape: (66000, 1748)\n",
      "Resized Wheat Data Shape: (53134, 1748)\n",
      "Combined Resized Spectral Data Shape: (122650, 1748)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import re\n",
    "\n",
    "# Define target size (e.g., size of the largest dataset)\n",
    "target_size = 1748\n",
    "\n",
    "# Function to resize spectra\n",
    "def resize_spectra(spectra, original_wavelengths, target_size):\n",
    "    resized_spectra = []\n",
    "    for spectrum in spectra:\n",
    "        interp_func = interp1d(original_wavelengths, spectrum, kind='linear', fill_value=\"extrapolate\")\n",
    "        new_wavelengths = np.linspace(original_wavelengths.min(), original_wavelengths.max(), target_size)\n",
    "        resized_spectrum = interp_func(new_wavelengths)\n",
    "        resized_spectra.append(resized_spectrum)\n",
    "    return np.array(resized_spectra)\n",
    "\n",
    "# Resize the ILS dataset\n",
    "original_wavelengths_ils = spectra_data.columns.to_series().apply(lambda x: float(re.findall(r'\\d+', x)[0]))\n",
    "spectra_ils_resized = resize_spectra(spectra_ils, original_wavelengths_ils, target_size)\n",
    "print(\"Resized ILS Data Shape:\", spectra_ils_resized.shape)\n",
    "\n",
    "# Resize the bacteria dataset\n",
    "original_wavelengths_bacteria = np.load(\"../data/bacteria/wavenumbers.npy\")\n",
    "X_bacteria_resized = resize_spectra(X_bacteria, original_wavelengths_bacteria, target_size)\n",
    "print(\"Resized Bacteria Data Shape:\", X_bacteria_resized.shape)\n",
    "\n",
    "# Resize the wheat dataset (already done)\n",
    "print(\"Resized Wheat Data Shape:\", spectra_wheat_resized.shape)\n",
    "\n",
    "# Combine resized spectral data\n",
    "X_combined_resized = np.vstack([spectra_ils_resized, X_bacteria_resized, spectra_wheat_resized])\n",
    "\n",
    "# Print the combined resized shape\n",
    "print(\"Combined Resized Spectral Data Shape:\", X_combined_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Labels Shape: (122650,)\n"
     ]
    }
   ],
   "source": [
    "# Combine labels\n",
    "y_combined = np.hstack([ils_target, y_bacteria, labels_encoded])\n",
    "\n",
    "# Print the combined labels shape\n",
    "print(\"Combined Labels Shape:\", y_combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## try model from suggested deepseek "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of labels in y_combined after conversion: [<class 'numpy.str_'>, <class 'numpy.str_'>, <class 'numpy.str_'>, <class 'numpy.str_'>, <class 'numpy.str_'>, <class 'numpy.str_'>, <class 'numpy.str_'>, <class 'numpy.str_'>, <class 'numpy.str_'>, <class 'numpy.str_'>]\n"
     ]
    }
   ],
   "source": [
    "# Convert all labels to strings\n",
    "y_combined = np.array([str(label) for label in y_combined])\n",
    "\n",
    "# Verify the types of the labels\n",
    "print(\"Types of labels in y_combined after conversion:\", [type(label) for label in y_combined[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in X_train: 66327\n",
      "Infinite values in X_train: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitsuko/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1327 - loss: 3.2290 - val_accuracy: 0.1544 - val_loss: 3.2057 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1511 - loss: 2.9564 - val_accuracy: 0.1372 - val_loss: 3.1011 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1485 - loss: 2.9215 - val_accuracy: 0.1543 - val_loss: 3.1759 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1518 - loss: 2.8873 - val_accuracy: 0.1428 - val_loss: 3.0954 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1552 - loss: 2.8763 - val_accuracy: 0.1556 - val_loss: 3.0248 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1557 - loss: 2.8674 - val_accuracy: 0.1384 - val_loss: 3.0202 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1579 - loss: 2.8605 - val_accuracy: 0.1531 - val_loss: 2.9754 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1554 - loss: 2.8549 - val_accuracy: 0.0375 - val_loss: 3.6815 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1559 - loss: 2.8457 - val_accuracy: 0.1556 - val_loss: 3.6759 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1604 - loss: 2.8551 - val_accuracy: 0.1549 - val_loss: 3.2625 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1559 - loss: 2.8429 - val_accuracy: 0.1559 - val_loss: 3.0765 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1565 - loss: 2.8390 - val_accuracy: 0.1576 - val_loss: 3.8850 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1592 - loss: 2.8318 - val_accuracy: 0.1556 - val_loss: 3.1703 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1602 - loss: 2.8251 - val_accuracy: 0.1448 - val_loss: 3.2667 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1605 - loss: 2.8366 - val_accuracy: 0.0391 - val_loss: 3.4864 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1594 - loss: 2.8322 - val_accuracy: 0.1446 - val_loss: 3.5693 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m1534/1534\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.1605 - loss: 2.8081 - val_accuracy: 0.1429 - val_loss: 3.2455 - learning_rate: 1.0000e-04\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - accuracy: 0.1536 - loss: 2.9650\n",
      "Test Loss: 2.9752962589263916\n",
      "Test Accuracy: 0.15108031034469604\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Check for NaN or infinite values\n",
    "print(\"NaN values in X_train:\", np.isnan(X_train).sum())\n",
    "print(\"Infinite values in X_train:\", np.isinf(X_train).sum())\n",
    "\n",
    "# Replace NaN and infinite values\n",
    "X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "# Standardize the spectral data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model with a lower learning rate and gradient clipping\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001, clipvalue=1.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train after fixing: (113426720, 34)\n",
      "Shape of y_val after fixing: (14178340, 34)\n",
      "Shape of y_test after fixing: (14178340, 34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitsuko/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/mitsuko/miniconda3/lib/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 98120\n'y' sizes: 113426720\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     52\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     55\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/data_adapter_utils.py:115\u001b[0m, in \u001b[0;36mcheck_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    111\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 98120\n'y' sizes: 113426720\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Standardize the spectral data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fix the shape of the labels\n",
    "y_train = y_train.reshape(-1, num_classes)  # Reshape to (num_samples, num_classes)\n",
    "y_val = y_val.reshape(-1, num_classes)  # Reshape to (num_samples, num_classes)\n",
    "y_test = y_test.reshape(-1, num_classes)  # Reshape to (num_samples, num_classes)\n",
    "\n",
    "# Verify the shapes\n",
    "print(\"Shape of y_train after fixing:\", y_train.shape)\n",
    "print(\"Shape of y_val after fixing:\", y_val.shape)\n",
    "print(\"Shape of y_test after fixing:\", y_test.shape)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1]))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. bacterial data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Initializing model...\n",
      "\n",
      "Starting training...\n",
      "Epoch: 1\n",
      "Train Loss: 0.4977 | Train Acc: 0.7732\n",
      "Val Loss: 3.6055 | Val Acc: 0.1257\n",
      "--------------------------------------------------\n",
      "Epoch: 2\n",
      "Train Loss: 0.2111 | Train Acc: 0.8644\n",
      "Val Loss: 3.8369 | Val Acc: 0.1343\n",
      "--------------------------------------------------\n",
      "Epoch: 3\n",
      "Train Loss: 0.1735 | Train Acc: 0.8838\n",
      "Val Loss: 3.7047 | Val Acc: 0.1620\n",
      "--------------------------------------------------\n",
      "Epoch: 4\n",
      "Train Loss: 0.1548 | Train Acc: 0.8933\n",
      "Val Loss: 4.3797 | Val Acc: 0.1500\n",
      "--------------------------------------------------\n",
      "Epoch: 5\n",
      "Train Loss: 0.1405 | Train Acc: 0.8999\n",
      "Val Loss: 4.1648 | Val Acc: 0.1220\n",
      "--------------------------------------------------\n",
      "Epoch: 6\n",
      "Train Loss: 0.1295 | Train Acc: 0.9060\n",
      "Val Loss: 4.1862 | Val Acc: 0.1160\n",
      "--------------------------------------------------\n",
      "Epoch: 7\n",
      "Train Loss: 0.1243 | Train Acc: 0.9097\n",
      "Val Loss: 3.9281 | Val Acc: 0.1390\n",
      "--------------------------------------------------\n",
      "Epoch: 8\n",
      "Train Loss: 0.1191 | Train Acc: 0.9124\n",
      "Val Loss: 4.1315 | Val Acc: 0.1407\n",
      "--------------------------------------------------\n",
      "Epoch: 9\n",
      "Train Loss: 0.1126 | Train Acc: 0.9149\n",
      "Val Loss: 4.2463 | Val Acc: 0.1320\n",
      "--------------------------------------------------\n",
      "Epoch: 10\n",
      "Train Loss: 0.1078 | Train Acc: 0.9180\n",
      "Val Loss: 3.9155 | Val Acc: 0.1237\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tb/4n3fh0qx0q734bc626dps61r0000gn/T/ipykernel_87264/687689181.py:249: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Accuracy: 0.4387\n",
      "\n",
      "Per-class accuracy:\n",
      "Accuracy of class 0: 75.00%\n",
      "Accuracy of class 1: 0.00%\n",
      "Accuracy of class 2: 1.00%\n",
      "Accuracy of class 3: 96.00%\n",
      "Accuracy of class 4: 37.00%\n",
      "Accuracy of class 5: 100.00%\n",
      "Accuracy of class 6: 57.00%\n",
      "Accuracy of class 7: 37.00%\n",
      "Accuracy of class 8: 0.00%\n",
      "Accuracy of class 9: 0.00%\n",
      "Accuracy of class 10: 34.00%\n",
      "Accuracy of class 11: 4.00%\n",
      "Accuracy of class 12: 9.00%\n",
      "Accuracy of class 13: 0.00%\n",
      "Accuracy of class 14: 96.00%\n",
      "Accuracy of class 15: 96.00%\n",
      "Accuracy of class 16: 4.00%\n",
      "Accuracy of class 17: 96.00%\n",
      "Accuracy of class 18: 94.00%\n",
      "Accuracy of class 19: 94.00%\n",
      "Accuracy of class 20: 83.00%\n",
      "Accuracy of class 21: 2.00%\n",
      "Accuracy of class 22: 1.00%\n",
      "Accuracy of class 23: 99.00%\n",
      "Accuracy of class 24: 0.00%\n",
      "Accuracy of class 25: 30.00%\n",
      "Accuracy of class 26: 34.00%\n",
      "Accuracy of class 27: 90.00%\n",
      "Accuracy of class 28: 35.00%\n",
      "Accuracy of class 29: 12.00%\n"
     ]
    }
   ],
   "source": [
    " import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ramanspy as rp\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "\n",
    "# Convert all labels to strings\n",
    "y_combined = np.array([str(label) for label in y_combined])\n",
    "\n",
    "# Encode the combined labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_combined_encoded = label_encoder.fit_transform(y_combined)\n",
    "\n",
    "# Convert the encoded labels to one-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_combined_one_hot = to_categorical(y_combined_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_combined_resized, y_combined_one_hot, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Focal Loss implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Self-attention module\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(in_dim, in_dim)\n",
    "        self.key = nn.Linear(in_dim, in_dim)\n",
    "        self.value = nn.Linear(in_dim, in_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(x.size(-1))\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attention, v)\n",
    "        return out\n",
    "\n",
    "# Enhanced network architecture\n",
    "class SpectraNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(SpectraNN, self).__init__()\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_bn = nn.BatchNorm1d(1000)\n",
    "        self.input_attention = SelfAttention(1000)\n",
    "        \n",
    "        # Deeper architecture with residual connections\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.attention = SelfAttention(512)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(128, 30)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_bn(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.input_attention(x)\n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.attention(x1.unsqueeze(1)).squeeze(1)\n",
    "        x = x1 + x2  # Residual connection\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Advanced data augmentation\n",
    "def augment_spectra(spectra, noise_level=0.02):\n",
    "    augmented = spectra.clone()\n",
    "    \n",
    "    # Random noise\n",
    "    noise = torch.randn_like(spectra) * noise_level\n",
    "    augmented += noise\n",
    "    \n",
    "    # Random scaling\n",
    "    scale = 1.0 + (torch.rand_like(spectra) * 0.1 - 0.05)\n",
    "    augmented *= scale\n",
    "    \n",
    "    # Random shifts\n",
    "    shift = (torch.rand_like(spectra) * 0.02 - 0.01)\n",
    "    augmented += shift\n",
    "    \n",
    "    # Random smoothing (randomly applied)\n",
    "    if torch.rand(1) > 0.5:\n",
    "        kernel_size = 3\n",
    "        pad_size = kernel_size // 2\n",
    "        padded = torch.nn.functional.pad(augmented.unsqueeze(1), (pad_size, pad_size), mode='reflect')\n",
    "        augmented = torch.nn.functional.avg_pool1d(padded, kernel_size=kernel_size, stride=1).squeeze(1)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train = rp.datasets.bacteria(\"train\", folder=\"../data/bacteria/\")\n",
    "X_test, y_test = rp.datasets.bacteria(\"test\", folder=\"../data/bacteria/\")\n",
    "X_val, y_val = rp.datasets.bacteria(\"val\", folder=\"../data/bacteria/\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = np.array(X_train.spectral_data)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test.spectral_data)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.array(X_val.spectral_data)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model and training components\n",
    "print(\"\\nInitializing model...\")\n",
    "model = SpectraNN()\n",
    "criterion = FocalLoss(gamma=2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "epochs = 10\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        # Multiple augmentations per batch\n",
    "        X_aug = torch.cat([augment_spectra(X_batch) for _ in range(2)])\n",
    "        y_aug = torch.cat([y_batch for _ in range(2)])\n",
    "        \n",
    "        outputs = model(X_aug)\n",
    "        loss = criterion(outputs, y_aug)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += y_aug.size(0)\n",
    "        train_correct += predicted.eq(y_aug).sum().item()\n",
    "        \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            outputs = model(X_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += predicted.eq(y_val).sum().item()\n",
    "    \n",
    "    # Print metrics\n",
    "    train_accuracy = train_correct / train_total\n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    print(f'Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {train_accuracy:.4f}')\n",
    "    print(f'Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {val_accuracy:.4f}')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "# Test evaluation with ensemble predictions\n",
    "print(\"\\nEvaluating final model...\")\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def ensemble_predict(model, X, n_augmentations=5):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        # Original prediction\n",
    "        pred = model(X)\n",
    "        predictions.append(pred)\n",
    "        \n",
    "        # Augmented predictions\n",
    "        for _ in range(n_augmentations-1):\n",
    "            aug_X = augment_spectra(X)\n",
    "            pred = model(aug_X)\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    return torch.stack(predictions).mean(0)\n",
    "\n",
    "# Final evaluation\n",
    "outputs = ensemble_predict(model, X_test_tensor)\n",
    "_, predicted = outputs.max(1)\n",
    "accuracy = predicted.eq(y_test_tensor).float().mean()\n",
    "print(f'\\nFinal Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Per-class accuracy\n",
    "class_correct = list(0. for i in range(30))\n",
    "class_total = list(0. for i in range(30))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(y_test_tensor)):\n",
    "        label = y_test_tensor[i]\n",
    "        class_correct[label] += (predicted[i] == label).item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "print('\\nPer-class accuracy:')\n",
    "for i in range(30):\n",
    "    if class_total[i] > 0:\n",
    "        print(f'Accuracy of class {i}: {100 * class_correct[i] / class_total[i]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from adenine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitsuko/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">223,872</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,122</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m223,872\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m)             │         \u001b[38;5;34m1,122\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,330</span> (919.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m235,330\u001b[0m (919.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,330</span> (919.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m235,330\u001b[0m (919.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1200 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 2/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1212 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 3/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1202 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 4/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1208 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 5/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1193 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 6/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1217 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 7/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1211 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 8/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1208 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 9/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1216 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 10/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1213 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 11/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1211 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 12/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1221 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 13/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1211 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 14/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1214 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 15/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1201 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 16/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1218 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 17/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.1203 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 18/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1204 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 19/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1219 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 20/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1203 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 21/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1205 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 22/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1211 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 23/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1225 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 24/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1220 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 25/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1222 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 26/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1195 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 27/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1188 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 28/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1200 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 29/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1200 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 30/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1208 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 31/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1207 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 32/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1223 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 33/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1200 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 34/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1197 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 35/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1210 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 36/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1196 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 37/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1201 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 38/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1210 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 39/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1221 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 40/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1219 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 41/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1189 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 42/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1199 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 43/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1193 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 44/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1217 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 45/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1204 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 46/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1196 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 47/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1200 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 48/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1219 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 49/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1211 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "Epoch 50/50\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.1217 - loss: nan - val_accuracy: 0.1222 - val_loss: nan\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - accuracy: 0.1194 - loss: nan\n",
      "Test Loss: nan\n",
      "Test Accuracy: 0.12303302437067032\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step\n",
      "Predicted: 0, Actual: 21\n",
      "Predicted: 0, Actual: 1\n",
      "Predicted: 0, Actual: 0\n",
      "Predicted: 0, Actual: 22\n",
      "Predicted: 0, Actual: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ramanspy as rp\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "\n",
    "# Convert all labels to strings\n",
    "y_combined = np.array([str(label) for label in y_combined])\n",
    "\n",
    "# Encode the combined labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_combined_encoded = label_encoder.fit_transform(y_combined)\n",
    "\n",
    "# Convert the encoded labels to one-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_combined_one_hot = to_categorical(y_combined_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and test (10%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_combined_resized, y_combined_one_hot, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))  # Dropout for regularization\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Output layer (for multi-class classification)\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model with a lower learning rate and gradient clipping\n",
    "model.compile(optimizer=Adam(learning_rate=0.001, clipvalue=1.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Decode the predicted labels back to substrate names\n",
    "predicted_substrates = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Compare predictions with actual values\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {predicted_substrates[i]}, Actual: {label_encoder.inverse_transform([np.argmax(y_test[i])])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
